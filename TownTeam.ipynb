{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "091f02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f19ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod_names 36\n",
      "reg_prices 46\n",
      "sale_prices 46\n",
      "sale_percents 36\n",
      "new_badges 18\n",
      "sizes 36\n",
      "colors 36\n"
     ]
    }
   ],
   "source": [
    "# Print the length of lists for debugging purposes\n",
    "print('prod_names',len(prod_names)) \n",
    "print('reg_prices',len(reg_prices))\n",
    "print('sale_prices',len(sale_prices))\n",
    "print('sale_percents',len(sale_percents))\n",
    "print('new_badges',len(new_badges))\n",
    "print('sizes',len(sizes))\n",
    "print('colors',len(colors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c9345622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "page switched\n",
      "pages ended\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store scraped data\n",
    "prod_name = []\n",
    "reg_price = []\n",
    "sale_price = []\n",
    "sale_percent = []\n",
    "new_badge = []\n",
    "size = []\n",
    "color = []\n",
    "link = []\n",
    "\n",
    "# Start scraping from page 1\n",
    "page_num = 1\n",
    "\n",
    "# Create an infinite loop to scrape multiple pages\n",
    "while True:\n",
    "    # Send a GET request to the website\n",
    "    result = requests.get(f'https://townteam.com/collections/summer-collection?filter.p.product_type=MEN&filter.p.product_type=Men+Belts&page={page_num}&sort_')\n",
    "    src = result.content\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    \n",
    "    # Check if we've reached the end of pages\n",
    "    if page_num > 493 // 20:\n",
    "        print('pages ended')\n",
    "        break\n",
    "    \n",
    "    # Extract data from the current page\n",
    "    prod_names = soup.find_all('div', {'class': 'card-summary card-list__hidden'})\n",
    "    reg_prices = soup.find_all('s', {'class': 'price-item price-item--regular'})\n",
    "    sale_prices = soup.find_all('span', {'class': 'price-item price-item--sale'})\n",
    "    sale_percents = soup.find_all('span', {'class': 'badge sale-badge'})\n",
    "    new_badges = soup.find_all('span', {'class': 'badge new-badge'})\n",
    "    sizes = soup.find_all('ul', {'class': 'sizes-list'})\n",
    "    colors = soup.find_all('span', {'data-change-title': True})\n",
    "    links = soup.find_all('a', {'class': 'button button-ATC'})\n",
    "    \n",
    "    # Loop through the extracted data\n",
    "    for i in range(len(prod_names)):\n",
    "        # Check if there are enough 'prod_names', otherwise assign a default value\n",
    "        if i < len(prod_names):\n",
    "            prod_name.append(prod_names[i].text.strip())\n",
    "        else:\n",
    "            prod_name.append('N/A')\n",
    "        \n",
    "        # Check if there are enough 'new_badges', otherwise assign a default value\n",
    "        if i < len(new_badges):\n",
    "            reg_price.append(reg_prices[i].text.strip())\n",
    "        else:\n",
    "            reg_price.append('N/A')\n",
    "        \n",
    "        # Check if there are enough 'sale_prices', otherwise assign a default value\n",
    "        if i < len(sale_prices):\n",
    "            sale_price.append(sale_prices[i].text.strip())\n",
    "        else:\n",
    "            sale_price.append('N/A')\n",
    "        \n",
    "        # Check if there are enough 'sale_percents', otherwise assign a default value\n",
    "        if i < len(sale_percents):\n",
    "            percenttxt = sale_percents[i].text.strip()\n",
    "            sale_percent.append(percenttxt[4:].strip())\n",
    "        else:\n",
    "            sale_percent.append('N/A')\n",
    "        \n",
    "        # Check if there are enough 'sizes', otherwise assign a default value\n",
    "        if i < len(sizes):\n",
    "            size.append(sizes[i].text.strip())\n",
    "        else:\n",
    "            size.append('N/A')\n",
    "        \n",
    "        # Check if there are enough 'colors', otherwise assign a default value\n",
    "        if i < len(colors):\n",
    "            colortxt = colors[i].text.strip()\n",
    "            color.append(colortxt[2:])\n",
    "        else:\n",
    "            color.append('N/A')\n",
    "        \n",
    "        # Check if there are enough 'links', otherwise assign a default value\n",
    "        if i < len(links):\n",
    "            link_complete = ('https://townteam.com/' + links[i].get('href'))\n",
    "            link.append(link_complete)\n",
    "        else:\n",
    "            link.append('N/A')\n",
    "    \n",
    "    page_num += 1\n",
    "    print('page switched')\n",
    "\n",
    "# Create a CSV file and fill it with the scraped values\n",
    "file_list = [prod_name, reg_price, sale_price, sale_percent, new_badge, size, color, link]\n",
    "exported = zip_longest(*file_list)\n",
    "with open(r'G:\\Courses\\Data science\\Role Models\\Codezilla\\Web Scraping/TownTeamScraping.csv', 'w', newline='', encoding='utf-8') as myfile:\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow(['prod_name', 'reg_price', 'sale_price', 'sale_percent', 'new_badge', 'size', 'color', 'Link'])\n",
    "    wr.writerows(exported)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c91de",
   "metadata": {},
   "source": [
    "### Author:\n",
    "### Ammar Allam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
